{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Krews, short for \u201cKotlin Reactive Workflows\u201d is a framework for creating scalable and reproducible scientific data pipelines using Docker Containers piped together with Project Reactor , a mature functional reactive programming library. Workflows are written using a Kotlin DSL in plain old Kotlin JVM projects, meaning you get all the benefits of a modern, type-safe, functional language with fantastic tooling. Quick Example \u00b6 fun main ( args : Array < String >) = run ( sampleWorkflow , args ) val sampleWorkflow = workflow ( \"sample-workflow\" ) { // Reactive \"Flux\" list object for the numbers 1 to 5 val range = ( 1. . 5 ). toFlux () task < Int , File >( \"base64\" , range ) { dockerImage = \"alpine:3.9\" output = OutputFile ( \"base64/$input.b64\" ) command = \"\"\" mkdir - p / data / base64 echo \"Hello World $input!\" | base64 > / data / base64 / $ input . b64 \"\"\" } } Configuration local { working-dir=/data } Run command java -jar my-app.jar --on local --config path/to/my-config.conf Supported Platforms \u00b6 Google Genomics Pipelines API Slurm Local Docker (Single Machine)","title":"Krews: Kotlin Reactive Workflows"},{"location":"#quick-example","text":"fun main ( args : Array < String >) = run ( sampleWorkflow , args ) val sampleWorkflow = workflow ( \"sample-workflow\" ) { // Reactive \"Flux\" list object for the numbers 1 to 5 val range = ( 1. . 5 ). toFlux () task < Int , File >( \"base64\" , range ) { dockerImage = \"alpine:3.9\" output = OutputFile ( \"base64/$input.b64\" ) command = \"\"\" mkdir - p / data / base64 echo \"Hello World $input!\" | base64 > / data / base64 / $ input . b64 \"\"\" } } Configuration local { working-dir=/data } Run command java -jar my-app.jar --on local --config path/to/my-config.conf","title":"Quick Example"},{"location":"#supported-platforms","text":"Google Genomics Pipelines API Slurm Local Docker (Single Machine)","title":"Supported Platforms"},{"location":"config/","text":"Configuration \u00b6 Krews requires configuration files to run. These files, along with command line arguments allow you to tweak how your workflow runs without touching or building your Krews application. The application itself is more concerned with what runs. In this way it is a means of separation of concerns . Configurations cover things like execution environment settings and parallelism. They also cover special parameters that you set up in your Krews Application to pass in differently for each run. They allow you to provide your workflow to 3 rd parties as an executable. The only thing your users would then need to worry about is creating a config. Krews configuration files are written in a superset of JSON called HOCON . It\u2019s just JSON with some additional syntax sugar that helps keep it concise and readable. These files conventionally use the extension .conf . There are two types of configurations, workflow scope and task scope. Workflow Configurations \u00b6 Workflow scope configurations apply to the entire workflow. They live at the top level of your configuration documents. Common Workflow Configurations \u00b6 The following are workflow configurations common for running against any execution environment. config name description required default parallelism The maximum number of tasks allowed to run concurrently. Set to an integer or \u201cunlimited\u201d no unlimited db-upload-delay Delay (in seconds) between uploading the latest database to project working storage no 60 report-generation-delay Delay (in seconds) between generating updated status reports no 60 Example parallelism = 10 db-upload-delay = 120 report-generation-delay = 180 Local Docker Execution Specific \u00b6 name description required default local.workingDir Working directory for local docker run no working-dir local.docker Docker client configurations. In most cases you should not need to set these. no none local.docker.uri URI of docker daemon no none local.docker.certificates-path Path to certificate for TLS enabled docker daemon no none local.docker.connect-timeout Timeout (in milliseconds) for connecting to docker daemon no 5000 local.docker.read-timeout Timeout (in milliseconds) for reading data from docker daemon no 30000 local.docker.connection-pool-size Connection pool size for docker client no 100 Example local { working-dir = some-test/directory docker { uri = \"localhost:1234\" certificate-path = \"~/my-path/my-cert.cert\" connect-timeout = 10000 read-timeout = 60000 connection-pool-size = 200 } } Google Cloud Execution Specific \u00b6 name description required default google.project-id Google Cloud project ID yes none google.regions List of regions in which we\u2019re allow to create VMs no none google.storage-bucket Google Cloud Storage bucket where we will be keeping our workflow files yes none google.storage-base-dir Google Cloud Storage base directory where we will be keeping our workflow files (in the given bucket) yes none google.job-completion-poll-interval Interval (in seconds) between checks for pipeline job completion no 10 google.log-upload-interval Interval (in seconds) between pipeline job log uploads to storage no 60 Example google { project-id = my-project regions = [us-east1, us-east2] storage-bucket = my-bucket storage-base-dir = my-base/directory job-completion-poll-interval = 60 log-upload-interval = 120 } Slurm Execution Specific \u00b6 name description required default slurm.working-dir Working directory for local docker run. Must be an NFS mounted directory accessible from workers yes none slurm.job-completion-poll-interval Interval (in seconds) between checks for pipeline job completion no 10 slurm.ssh Used to connect to Slurm head node to run sbatch jobs and poll job status. Enables running remote machines that can ssh to slurm head node with passwordless login no none slurm.ssh.user User for Slurm head node ssh no none slurm.ssh.host Slurm head node host name no none slurm.ssh.port Slurm head node ssh port no 22 Example slurm { working-dir = /data/my-dir job-completion-poll-interval = 30 ssh { user = jbrooks host = slurm018 port = 23 } } Task Configurations \u00b6 Task scope configurations apply to only individual tasks. They live under special sections of your configuration files. // task configs in this section apply to ALL tasks task.default { //... } // task configs in this section apply to tasks with a name matching \"my-task-name\" task.my-task-name { //... } // task configs in this section apply to tasks with a label matching \"my-task-label\" task.my-task-label { //... } Some of these sections match our application tasks based on names and labels. These are based on fields you use when you create your tasks. task < MyTaskInput >( \"my-task-name\" , \"my-task-label\" , \"my-other-task-label\" ) { // ... } Note that these fields must be hyphenated-snake-case but names. Since labels are free-form string fields, if you use a string like \u201cMy Task.Name!\u201d, it will automatically be converted to deserialize from \u201cmy-task-name\u201d. It is recommended you make all task names and labels hyphenated-snake-case to match configurations. Task configurations will be merged for each task before deserializing, so you could set one setting in task.default , one in task.my-task-name , and one in task.my-task-label they could all be passed into a task. Common Task Configurations \u00b6 name description required default parallelism The maximum number of this task that can run concurrently. Set to an integer or \u201cunlimited.\u201d no unlimited Example task.my-task-name { parallelism = 10 } Google Cloud Execution Specific \u00b6 name description required default google.machine-type A native google compute engine machine type (See here for complete list). If this is set, it\u2019s always used regardless of other configs. no none google.machine-class A class of machine. Useful for when you don\u2019t know the needed resources until runtime. Options are: standard, highmem, highcpu, ultramem, custom no none google.cpus Number of CPUs. Can be used to override the runtime value. no none google.mem Amount of memory. Can be used to override the runtime value. no none google.mem-per-cpu An optional memory per cpu ratio used when you don\u2019t have both fields available and don\u2019t want to use a machine class. no none google.disk-size Disk capacity. Can be used to override the runtime value. no none google.disk-type Type of disk, HDD vs SSD. no hdd Example task.my-task-name { google { machine-type = n1-standard-2 machine-class = standard cpus = 2 mem = 8GB mem-per-cpu = 2GB disk-size = 1TB disk-type = ssd } } Slurm Execution Specific \u00b6 name description required default slurm.cpus Number of cpus. Can be used to override the runtime value. no none slurm.mem Amount of memory. Can be used to override the runtime value. no none slurm.time Time limit on the run time for the job in minutes. no none slurm.partition SBatch partition to use. no none slurm.sbatch-args Additional sbatch args used with our sbatch commands to initiate jobs. See reference . no none Example task.my-task-name { slurm { cpus = 4 mem = 16GB time = 120 partition = my-partition sbatch-args = \"--nodes=2 --overcommit -priority=TOP\" } } Parameters \u00b6 As mentioned in the workflows section, workflows and tasks have parameters customized in your application that get injected from configuration files. These are registered in your application as classes. The process of turning JSON (and HOCON) into objects of these classes is called deserialization. We do this using the Jackson library. Workflow Parameters \u00b6 If your workflow contains the following data class SampleParams ( val message : String , // The ? means this is optional and will be null by default val flag : Boolean ?, // This has a default value of 10, so we don't actually need to set it in our config val rangeMax : Int = 10 ) val sampleWorkflow = workflow ( \"sample-workflow\" ) { val params = params < SampleParams >() //... } You will be able to inject the following as parameters params { message = \"Hello World!\" range-max = 5 } Notice that the rangeMax data class field was automatically converted to a hyphenated-snake-case version range-max . This will happen for all params. Task Parameters \u00b6 Just like for workflows, we have custom parameters that can be injected into each task. data class MyTaskParams ( val someSetting : Boolean ) task < MyTaskInput >( \"my-task-name\" , \"my-task-label\" , \"my-other-task-label\" ) { val taskParams = taskParams < MyTaskParams > //... } task.my-task-name { params { some-setting = true } } Ambiguity in Deserialization \u00b6 For abstract classes and interfaces, we have special considerations in deserialization. Consider the following: // In model/Pet.kt interface Pet data class Dog ( val barkVolume : Int ) : Pet data class Cat ( val meowVolume : Int ) : Pet // In App.kt data class SampleParams ( val pet : Pet ) val sampleWorkflow = workflow ( \"sample-workflow\" ) { val params = params < SampleParams >() //... } Since the pet parameter in HOCON is a type that is not concrete, Krews allows you to provide the concrete type. params { pet { -type = \"model.Dog\" bark-volume = 50 } } Input Files as Parameters \u00b6 One of the most common uses of params is to provide files for each run. These files will be provided as InputFiles. The InputFile class is abstract, so when we pass them in we need to pass in implementations like GSInputFile for files in Google Cloud Storage and LocalInputFile for files on your local file system. Here\u2019s an example params for a list of InputFiles data class SampleParams ( val myFiles : List < InputFile >) val sampleWorkflow = workflow ( \"sample-workflow\" ) { val params = params < SampleParams >() //... } params { my-files = [ { -type = \"krews.file.GSInputFile\" bucket = \"my-bucket\" object-path = \"some/object/path/file1.tar\" path = \"path/file1.tar\" }, { -type = \"krews.file.GSInputFile\" bucket = \"my-bucket\" object-path = \"some/object/path/file2.tar\" path = \"path/file2.tar\" } ] }","title":"Configuration Basics"},{"location":"config/#configuration","text":"Krews requires configuration files to run. These files, along with command line arguments allow you to tweak how your workflow runs without touching or building your Krews application. The application itself is more concerned with what runs. In this way it is a means of separation of concerns . Configurations cover things like execution environment settings and parallelism. They also cover special parameters that you set up in your Krews Application to pass in differently for each run. They allow you to provide your workflow to 3 rd parties as an executable. The only thing your users would then need to worry about is creating a config. Krews configuration files are written in a superset of JSON called HOCON . It\u2019s just JSON with some additional syntax sugar that helps keep it concise and readable. These files conventionally use the extension .conf . There are two types of configurations, workflow scope and task scope.","title":"Configuration"},{"location":"config/#workflow-configurations","text":"Workflow scope configurations apply to the entire workflow. They live at the top level of your configuration documents.","title":"Workflow Configurations"},{"location":"config/#common-workflow-configurations","text":"The following are workflow configurations common for running against any execution environment. config name description required default parallelism The maximum number of tasks allowed to run concurrently. Set to an integer or \u201cunlimited\u201d no unlimited db-upload-delay Delay (in seconds) between uploading the latest database to project working storage no 60 report-generation-delay Delay (in seconds) between generating updated status reports no 60 Example parallelism = 10 db-upload-delay = 120 report-generation-delay = 180","title":"Common Workflow Configurations"},{"location":"config/#local-docker-execution-specific","text":"name description required default local.workingDir Working directory for local docker run no working-dir local.docker Docker client configurations. In most cases you should not need to set these. no none local.docker.uri URI of docker daemon no none local.docker.certificates-path Path to certificate for TLS enabled docker daemon no none local.docker.connect-timeout Timeout (in milliseconds) for connecting to docker daemon no 5000 local.docker.read-timeout Timeout (in milliseconds) for reading data from docker daemon no 30000 local.docker.connection-pool-size Connection pool size for docker client no 100 Example local { working-dir = some-test/directory docker { uri = \"localhost:1234\" certificate-path = \"~/my-path/my-cert.cert\" connect-timeout = 10000 read-timeout = 60000 connection-pool-size = 200 } }","title":"Local Docker Execution Specific"},{"location":"config/#google-cloud-execution-specific","text":"name description required default google.project-id Google Cloud project ID yes none google.regions List of regions in which we\u2019re allow to create VMs no none google.storage-bucket Google Cloud Storage bucket where we will be keeping our workflow files yes none google.storage-base-dir Google Cloud Storage base directory where we will be keeping our workflow files (in the given bucket) yes none google.job-completion-poll-interval Interval (in seconds) between checks for pipeline job completion no 10 google.log-upload-interval Interval (in seconds) between pipeline job log uploads to storage no 60 Example google { project-id = my-project regions = [us-east1, us-east2] storage-bucket = my-bucket storage-base-dir = my-base/directory job-completion-poll-interval = 60 log-upload-interval = 120 }","title":"Google Cloud Execution Specific"},{"location":"config/#slurm-execution-specific","text":"name description required default slurm.working-dir Working directory for local docker run. Must be an NFS mounted directory accessible from workers yes none slurm.job-completion-poll-interval Interval (in seconds) between checks for pipeline job completion no 10 slurm.ssh Used to connect to Slurm head node to run sbatch jobs and poll job status. Enables running remote machines that can ssh to slurm head node with passwordless login no none slurm.ssh.user User for Slurm head node ssh no none slurm.ssh.host Slurm head node host name no none slurm.ssh.port Slurm head node ssh port no 22 Example slurm { working-dir = /data/my-dir job-completion-poll-interval = 30 ssh { user = jbrooks host = slurm018 port = 23 } }","title":"Slurm Execution Specific"},{"location":"config/#task-configurations","text":"Task scope configurations apply to only individual tasks. They live under special sections of your configuration files. // task configs in this section apply to ALL tasks task.default { //... } // task configs in this section apply to tasks with a name matching \"my-task-name\" task.my-task-name { //... } // task configs in this section apply to tasks with a label matching \"my-task-label\" task.my-task-label { //... } Some of these sections match our application tasks based on names and labels. These are based on fields you use when you create your tasks. task < MyTaskInput >( \"my-task-name\" , \"my-task-label\" , \"my-other-task-label\" ) { // ... } Note that these fields must be hyphenated-snake-case but names. Since labels are free-form string fields, if you use a string like \u201cMy Task.Name!\u201d, it will automatically be converted to deserialize from \u201cmy-task-name\u201d. It is recommended you make all task names and labels hyphenated-snake-case to match configurations. Task configurations will be merged for each task before deserializing, so you could set one setting in task.default , one in task.my-task-name , and one in task.my-task-label they could all be passed into a task.","title":"Task Configurations"},{"location":"config/#common-task-configurations","text":"name description required default parallelism The maximum number of this task that can run concurrently. Set to an integer or \u201cunlimited.\u201d no unlimited Example task.my-task-name { parallelism = 10 }","title":"Common Task Configurations"},{"location":"config/#google-cloud-execution-specific_1","text":"name description required default google.machine-type A native google compute engine machine type (See here for complete list). If this is set, it\u2019s always used regardless of other configs. no none google.machine-class A class of machine. Useful for when you don\u2019t know the needed resources until runtime. Options are: standard, highmem, highcpu, ultramem, custom no none google.cpus Number of CPUs. Can be used to override the runtime value. no none google.mem Amount of memory. Can be used to override the runtime value. no none google.mem-per-cpu An optional memory per cpu ratio used when you don\u2019t have both fields available and don\u2019t want to use a machine class. no none google.disk-size Disk capacity. Can be used to override the runtime value. no none google.disk-type Type of disk, HDD vs SSD. no hdd Example task.my-task-name { google { machine-type = n1-standard-2 machine-class = standard cpus = 2 mem = 8GB mem-per-cpu = 2GB disk-size = 1TB disk-type = ssd } }","title":"Google Cloud Execution Specific"},{"location":"config/#slurm-execution-specific_1","text":"name description required default slurm.cpus Number of cpus. Can be used to override the runtime value. no none slurm.mem Amount of memory. Can be used to override the runtime value. no none slurm.time Time limit on the run time for the job in minutes. no none slurm.partition SBatch partition to use. no none slurm.sbatch-args Additional sbatch args used with our sbatch commands to initiate jobs. See reference . no none Example task.my-task-name { slurm { cpus = 4 mem = 16GB time = 120 partition = my-partition sbatch-args = \"--nodes=2 --overcommit -priority=TOP\" } }","title":"Slurm Execution Specific"},{"location":"config/#parameters","text":"As mentioned in the workflows section, workflows and tasks have parameters customized in your application that get injected from configuration files. These are registered in your application as classes. The process of turning JSON (and HOCON) into objects of these classes is called deserialization. We do this using the Jackson library.","title":"Parameters"},{"location":"config/#workflow-parameters","text":"If your workflow contains the following data class SampleParams ( val message : String , // The ? means this is optional and will be null by default val flag : Boolean ?, // This has a default value of 10, so we don't actually need to set it in our config val rangeMax : Int = 10 ) val sampleWorkflow = workflow ( \"sample-workflow\" ) { val params = params < SampleParams >() //... } You will be able to inject the following as parameters params { message = \"Hello World!\" range-max = 5 } Notice that the rangeMax data class field was automatically converted to a hyphenated-snake-case version range-max . This will happen for all params.","title":"Workflow Parameters"},{"location":"config/#task-parameters","text":"Just like for workflows, we have custom parameters that can be injected into each task. data class MyTaskParams ( val someSetting : Boolean ) task < MyTaskInput >( \"my-task-name\" , \"my-task-label\" , \"my-other-task-label\" ) { val taskParams = taskParams < MyTaskParams > //... } task.my-task-name { params { some-setting = true } }","title":"Task Parameters"},{"location":"config/#ambiguity-in-deserialization","text":"For abstract classes and interfaces, we have special considerations in deserialization. Consider the following: // In model/Pet.kt interface Pet data class Dog ( val barkVolume : Int ) : Pet data class Cat ( val meowVolume : Int ) : Pet // In App.kt data class SampleParams ( val pet : Pet ) val sampleWorkflow = workflow ( \"sample-workflow\" ) { val params = params < SampleParams >() //... } Since the pet parameter in HOCON is a type that is not concrete, Krews allows you to provide the concrete type. params { pet { -type = \"model.Dog\" bark-volume = 50 } }","title":"Ambiguity in Deserialization"},{"location":"config/#input-files-as-parameters","text":"One of the most common uses of params is to provide files for each run. These files will be provided as InputFiles. The InputFile class is abstract, so when we pass them in we need to pass in implementations like GSInputFile for files in Google Cloud Storage and LocalInputFile for files on your local file system. Here\u2019s an example params for a list of InputFiles data class SampleParams ( val myFiles : List < InputFile >) val sampleWorkflow = workflow ( \"sample-workflow\" ) { val params = params < SampleParams >() //... } params { my-files = [ { -type = \"krews.file.GSInputFile\" bucket = \"my-bucket\" object-path = \"some/object/path/file1.tar\" path = \"path/file1.tar\" }, { -type = \"krews.file.GSInputFile\" bucket = \"my-bucket\" object-path = \"some/object/path/file2.tar\" path = \"path/file2.tar\" } ] }","title":"Input Files as Parameters"},{"location":"docker/","text":"Docker \u00b6 Docker is a program for building and running software packages called containers . Containers are a great way of making sure your code runs the same way everywhere without worrying about installing or configuring anything. For this reason Krews uses docker containers for everything. Krews does not even attempt to without containers, nor are there plans to try in the future. How Krews uses Docker \u00b6 Krews runs each task in separate Docker containers. Each execution environment has different ways of running and accessing docker containers and different ways of storing workflow files (state, results, logs, etc\u2026) but running a task typically looks like this: A directory is created that will be mounted inside a docker container (to /data by default) Files are copied into this directory from file storage Krews executes the docker container with a specified command. Metadata about the run is updated and stored in the background. Krews polls until the command execution is complete. Files are copied out of the mounted directory back to File Storage File Storage \u00b6 File Storage and the methods that files are copied vary per execution environment. For example, the Google executor keeps data in Google Cloud Storage whereas the Slurm executor relies on an NFS that available to all worker nodes. What about Singularity? \u00b6 The Krews Slurm executor uses Singularity to run docker containers. Krews does not support singularity containers.","title":"How Krews uses Docker"},{"location":"docker/#docker","text":"Docker is a program for building and running software packages called containers . Containers are a great way of making sure your code runs the same way everywhere without worrying about installing or configuring anything. For this reason Krews uses docker containers for everything. Krews does not even attempt to without containers, nor are there plans to try in the future.","title":"Docker"},{"location":"docker/#how-krews-uses-docker","text":"Krews runs each task in separate Docker containers. Each execution environment has different ways of running and accessing docker containers and different ways of storing workflow files (state, results, logs, etc\u2026) but running a task typically looks like this: A directory is created that will be mounted inside a docker container (to /data by default) Files are copied into this directory from file storage Krews executes the docker container with a specified command. Metadata about the run is updated and stored in the background. Krews polls until the command execution is complete. Files are copied out of the mounted directory back to File Storage","title":"How Krews uses Docker"},{"location":"docker/#file-storage","text":"File Storage and the methods that files are copied vary per execution environment. For example, the Google executor keeps data in Google Cloud Storage whereas the Slurm executor relies on an NFS that available to all worker nodes.","title":"File Storage"},{"location":"docker/#what-about-singularity","text":"The Krews Slurm executor uses Singularity to run docker containers. Krews does not support singularity containers.","title":"What about Singularity?"},{"location":"quickstart/","text":"Quick Start \u00b6 The fastest way to get started is to clone the krews-boilerplate Creating a Project \u00b6 Instructions on setting up Kotlin projects can be found here . Installing Krews Dependency \u00b6 You can get the Krews library using any Maven compatible build system Gradle \u00b6 dependencies { compile ( \"io.krews\" , \"krews\" , \"0.5.18\" ) } Maven \u00b6 <dependency> <groupId> io.krews </groupId> <artifactId> krews </artifactId> <version> 0.5.18 </version> </dependency> Building \u00b6 It\u2019s highly recommended that you build the application into an executable Jar. On Gradle, this can be done using the Shadow Plugin On Maven, this can be done using the Shade Plugin If you\u2019d like to use your project as a library to be referenced in other projects / workflows, you\u2019ll need to also create a sources Jar. Publishing your Jars to a Maven Repository like Bintray will give you a free (for open source) place to store your libraries and executables.","title":"Creating a Project"},{"location":"quickstart/#quick-start","text":"The fastest way to get started is to clone the krews-boilerplate","title":"Quick Start"},{"location":"quickstart/#creating-a-project","text":"Instructions on setting up Kotlin projects can be found here .","title":"Creating a Project"},{"location":"quickstart/#installing-krews-dependency","text":"You can get the Krews library using any Maven compatible build system","title":"Installing Krews Dependency"},{"location":"quickstart/#gradle","text":"dependencies { compile ( \"io.krews\" , \"krews\" , \"0.5.18\" ) }","title":"Gradle"},{"location":"quickstart/#maven","text":"<dependency> <groupId> io.krews </groupId> <artifactId> krews </artifactId> <version> 0.5.18 </version> </dependency>","title":"Maven"},{"location":"quickstart/#building","text":"It\u2019s highly recommended that you build the application into an executable Jar. On Gradle, this can be done using the Shadow Plugin On Maven, this can be done using the Shade Plugin If you\u2019d like to use your project as a library to be referenced in other projects / workflows, you\u2019ll need to also create a sources Jar. Publishing your Jars to a Maven Repository like Bintray will give you a free (for open source) place to store your libraries and executables.","title":"Building"},{"location":"reactor/","text":"Reactor \u00b6 Project Reactor is a functional reactive programming library, consisting of \u201cpublishers\u201d and \u201coperations\u201d Publishers, Fluxes, and Monos \u00b6 A Publisher is just a higher level interface for both Fluxes and Monos. You can think of them as streams. They are more sophisticated than traditional streams, notably in operating by push and pull, but let\u2019s not worry about that for now. A Flux is just a Publisher with multiple elements. We will mostly be dealing with these. A Mono is just a Publisher with one element. Flux and Mono Creation \u00b6 Fluxes and Monos can be created with the toFlux() and toMono() kotlin extension functions. More on that here import reactor.core.publisher.* val flux1 = listOf ( \"a\" , \"b\" , \"c\" ). toFlux () Operations \u00b6 Operations are the functional part of functional reactive programming . Every Publisher has functions that transform it into one or more other publishers. import reactor.core.publisher.* val flux1 = listOf ( \"a\" , \"b\" , \"c\" ). toFlux () val flux2 = flux1 . map { \"$it$it\" } // \"aa\", \"bb\", \"cc\" There are many operators. A full list for Fluxes can be found here and for Monos here .","title":"Publishers, Fluxes, and Monos"},{"location":"reactor/#reactor","text":"Project Reactor is a functional reactive programming library, consisting of \u201cpublishers\u201d and \u201coperations\u201d","title":"Reactor"},{"location":"reactor/#publishers-fluxes-and-monos","text":"A Publisher is just a higher level interface for both Fluxes and Monos. You can think of them as streams. They are more sophisticated than traditional streams, notably in operating by push and pull, but let\u2019s not worry about that for now. A Flux is just a Publisher with multiple elements. We will mostly be dealing with these. A Mono is just a Publisher with one element.","title":"Publishers, Fluxes, and Monos"},{"location":"reactor/#flux-and-mono-creation","text":"Fluxes and Monos can be created with the toFlux() and toMono() kotlin extension functions. More on that here import reactor.core.publisher.* val flux1 = listOf ( \"a\" , \"b\" , \"c\" ). toFlux ()","title":"Flux and Mono Creation"},{"location":"reactor/#operations","text":"Operations are the functional part of functional reactive programming . Every Publisher has functions that transform it into one or more other publishers. import reactor.core.publisher.* val flux1 = listOf ( \"a\" , \"b\" , \"c\" ). toFlux () val flux2 = flux1 . map { \"$it$it\" } // \"aa\", \"bb\", \"cc\" There are many operators. A full list for Fluxes can be found here and for Monos here .","title":"Operations"},{"location":"running_environments/","text":"Running / Environments \u00b6 Running Krews \u00b6 To run Krews as built as an executable, use: java -jar my-app.jar --on google --config path/to/my-config.conf The following is a complete list of arguments for Krews applications. Name Description Required --on Execution environment. See below for all types and explanations. yes --conf Configuration file. See configurations section for more. yes Google Cloud \u00b6 Krews uses the Google Cloud Genomics Pipelines API to run on Google Cloud. In a nutshell, this API is just a service that creates custom VMs on-the-fly to run docker containers. Our implementation stores files on Google Cloud Storage. Krews actually has two different ways it can run on Google Cloud, directed locally and directed remotely in the cloud. Remote Execution \u00b6 When Krews runs in remote execution mode on Google Cloud, your Krews application executable and configuration are copied into Google Cloud Storage, and the Pipelines API is used to spin up a machine to download and run your Krews application in the cloud. To run on Google Cloud in remote execution mode use: java -jar my-app.jar --on google --config path/to/my-config.conf Local Execution \u00b6 When Krews run in local execution mode, keep in mind that the application runs for the entire duration of your workflow and must be uninterrupted. You probably do not want to do this on a laptop. To run on Google Cloud in locally directed mode use: java -jar my-app.jar --on google-local --config path/to/my-config.conf Slurm \u00b6 Krews\u2019 Slurm Executor runs on compute clusters as Slurm sbatch Jobs. Each job is run using Singularity , which must be installed on every Slurm worker node. Files are stored on an NFS mounted directory accessible to your Krews application and the jobs running on Slurm worker nodes. krews does not need to be run on a Slurm head node. It just needs to be able to ssh into one without using a password. For more on this see configs . To run on Slurm use: java -jar my-app.jar --on slurm --config path/to/my-config.conf Local Docker \u00b6 Krews\u2019 Local Docker Executor runs all jobs on Docker running on the same machine as Krews. It stores files on any given locally accessible file system directory. To run locally use: java -jar my-app.jar --on local --config path/to/my-config.conf","title":"Running"},{"location":"running_environments/#running-environments","text":"","title":"Running / Environments"},{"location":"running_environments/#running-krews","text":"To run Krews as built as an executable, use: java -jar my-app.jar --on google --config path/to/my-config.conf The following is a complete list of arguments for Krews applications. Name Description Required --on Execution environment. See below for all types and explanations. yes --conf Configuration file. See configurations section for more. yes","title":"Running Krews"},{"location":"running_environments/#google-cloud","text":"Krews uses the Google Cloud Genomics Pipelines API to run on Google Cloud. In a nutshell, this API is just a service that creates custom VMs on-the-fly to run docker containers. Our implementation stores files on Google Cloud Storage. Krews actually has two different ways it can run on Google Cloud, directed locally and directed remotely in the cloud.","title":"Google Cloud"},{"location":"running_environments/#remote-execution","text":"When Krews runs in remote execution mode on Google Cloud, your Krews application executable and configuration are copied into Google Cloud Storage, and the Pipelines API is used to spin up a machine to download and run your Krews application in the cloud. To run on Google Cloud in remote execution mode use: java -jar my-app.jar --on google --config path/to/my-config.conf","title":"Remote Execution"},{"location":"running_environments/#local-execution","text":"When Krews run in local execution mode, keep in mind that the application runs for the entire duration of your workflow and must be uninterrupted. You probably do not want to do this on a laptop. To run on Google Cloud in locally directed mode use: java -jar my-app.jar --on google-local --config path/to/my-config.conf","title":"Local Execution"},{"location":"running_environments/#slurm","text":"Krews\u2019 Slurm Executor runs on compute clusters as Slurm sbatch Jobs. Each job is run using Singularity , which must be installed on every Slurm worker node. Files are stored on an NFS mounted directory accessible to your Krews application and the jobs running on Slurm worker nodes. krews does not need to be run on a Slurm head node. It just needs to be able to ssh into one without using a password. For more on this see configs . To run on Slurm use: java -jar my-app.jar --on slurm --config path/to/my-config.conf","title":"Slurm"},{"location":"running_environments/#local-docker","text":"Krews\u2019 Local Docker Executor runs all jobs on Docker running on the same machine as Krews. It stores files on any given locally accessible file system directory. To run locally use: java -jar my-app.jar --on local --config path/to/my-config.conf","title":"Local Docker"},{"location":"state/","text":"Output and State \u00b6 Caching \u00b6 Tasks can often be expensive, so we don\u2019t want to repeat them if we don\u2019t need to. That\u2019s where caching comes in. For every file created by a task run found in the \u201coutputs\u201d directory, we save the following data related to the file and task runs that created it: File path File last modified time (for tracking changes) Task name Docker Image Command Inputs (as JSON) Params (as JSON) For each task run, if these fields match was we have saved and the file still exists in the outputs directory with the same last modified time, we skip the running the task. State File \u00b6 Krews keeps it\u2019s cache and some other metadata in a SQLite file database. This file is periodically copied into your working directory under state/metadata.db. It should NOT be deleted if you want to make full use of caching. Reports \u00b6 As tasks are run, you can go into your working directory and find your status/report.html file. It will contain information on every task that was run, including whether it completed successfully, in error, or is still in progress. Workflow Output Layout \u00b6 Although each execution environment stores files differently, the way they layout outputs for the workflow is mostly the same. The the top level in the working directory (provided in config) we have the following outputs/ contains files created by our tasks. state/ contains a file with our cache data. output/ contains all files related to a single run. Each output run directory looks like the following bin/ contains the Krews workflow executable and configuration used for this run. status/ contains an html report on the status of this run. logs/ contains logs for each task and the Krews application itself, if run remotely in Google Cloud.","title":"Caching"},{"location":"state/#output-and-state","text":"","title":"Output and State"},{"location":"state/#caching","text":"Tasks can often be expensive, so we don\u2019t want to repeat them if we don\u2019t need to. That\u2019s where caching comes in. For every file created by a task run found in the \u201coutputs\u201d directory, we save the following data related to the file and task runs that created it: File path File last modified time (for tracking changes) Task name Docker Image Command Inputs (as JSON) Params (as JSON) For each task run, if these fields match was we have saved and the file still exists in the outputs directory with the same last modified time, we skip the running the task.","title":"Caching"},{"location":"state/#state-file","text":"Krews keeps it\u2019s cache and some other metadata in a SQLite file database. This file is periodically copied into your working directory under state/metadata.db. It should NOT be deleted if you want to make full use of caching.","title":"State File"},{"location":"state/#reports","text":"As tasks are run, you can go into your working directory and find your status/report.html file. It will contain information on every task that was run, including whether it completed successfully, in error, or is still in progress.","title":"Reports"},{"location":"state/#workflow-output-layout","text":"Although each execution environment stores files differently, the way they layout outputs for the workflow is mostly the same. The the top level in the working directory (provided in config) we have the following outputs/ contains files created by our tasks. state/ contains a file with our cache data. output/ contains all files related to a single run. Each output run directory looks like the following bin/ contains the Krews workflow executable and configuration used for this run. status/ contains an html report on the status of this run. logs/ contains logs for each task and the Krews application itself, if run remotely in Google Cloud.","title":"Workflow Output Layout"},{"location":"why_krews/","text":"Why Krews? \u00b6 Tooling support \u00b6 Since Krews is written in plain old Kotlin, you get access to the great IDEs and editors that already support it. We recommend Intellij IDEA . You can also use any build tools that support Kotlin. We recommend Gradle or Maven . Clean, Concise, and Powerful \u00b6 Written using Kotlin\u2019s DSL capabilities, Krews applications are semi-declarative. This means they\u2019re laid out like simple configurations, but open to any amount of programmatic customization. Because we also use standard Kotlin projects, Krews code can and should be split up and organized into intuitive file structures. See the Boilerplate for an example of this. Docker Native \u00b6 Docker containerization is not only supported, it is required. We do not support running workflow tasks outside containers. This means your workflows will always be as reproducible as possible and anyone who uses your workflow or even just your containers will not have to worry about installing amd configuring dependencies. Cloud Native \u00b6 Krews currently supports the Google Genomics Pipelines API as a first class citizen. It was the first integration written, and support was not shoe-horned into a model that does not support it well. It will take full advantage of the Pipelines API\u2019s ability to create and use exactly the VM you need for every task. Not only that, but with the Pipelines API Krews can automatically create a \u201cMaster\u201d VM, copy itself to the VM (via Google Cloud Storage), and run there completely. Logs for the master process and each krews task, Krews\u2019 state file (an SQLite file), run reports, and outputs will be automatically managed and organized in a manner that is easy to navigate and debug.","title":"Tooling"},{"location":"why_krews/#why-krews","text":"","title":"Why Krews?"},{"location":"why_krews/#tooling-support","text":"Since Krews is written in plain old Kotlin, you get access to the great IDEs and editors that already support it. We recommend Intellij IDEA . You can also use any build tools that support Kotlin. We recommend Gradle or Maven .","title":"Tooling support"},{"location":"why_krews/#clean-concise-and-powerful","text":"Written using Kotlin\u2019s DSL capabilities, Krews applications are semi-declarative. This means they\u2019re laid out like simple configurations, but open to any amount of programmatic customization. Because we also use standard Kotlin projects, Krews code can and should be split up and organized into intuitive file structures. See the Boilerplate for an example of this.","title":"Clean, Concise, and Powerful"},{"location":"why_krews/#docker-native","text":"Docker containerization is not only supported, it is required. We do not support running workflow tasks outside containers. This means your workflows will always be as reproducible as possible and anyone who uses your workflow or even just your containers will not have to worry about installing amd configuring dependencies.","title":"Docker Native"},{"location":"why_krews/#cloud-native","text":"Krews currently supports the Google Genomics Pipelines API as a first class citizen. It was the first integration written, and support was not shoe-horned into a model that does not support it well. It will take full advantage of the Pipelines API\u2019s ability to create and use exactly the VM you need for every task. Not only that, but with the Pipelines API Krews can automatically create a \u201cMaster\u201d VM, copy itself to the VM (via Google Cloud Storage), and run there completely. Logs for the master process and each krews task, Krews\u2019 state file (an SQLite file), run reports, and outputs will be automatically managed and organized in a manner that is easy to navigate and debug.","title":"Cloud Native"},{"location":"workflows/","text":"Workflows \u00b6 A \u201cWorkflow\u201d is a directed acyclic graph consisting of \u201cTasks\u201d glued together with publishers. The following simple example contains two tasks. import krews.core.* import krews.file.* import krews.run import reactor.core.publisher.* // Bootstrap fun main ( args : Array < String >) = run ( sampleWorkflow , args ) // The application's workflow, named \"sample-workflow\" val sampleWorkflow = workflow ( \"sample-workflow\" ) { // A simple flux of the integers 1 through 10 val range = ( 1. . 10 ). toFlux () // A task that creates base64 files val base64 = task < Int , File >( \"base64\" , range ) { dockerImage = \"alpine:3.8\" output = OutputFile ( \"base64/$input.b64\" ) command = \"\"\" mkdir - p / data / base64 echo \"Hello world number $$input!\" | base64 > / data / base64 / $ input . b64 \"\"\" } // A task that zips files task < File , File >( \"gzip\" , base64 . outputPub ) { dockerImage = \"alpine:3.8\" output = OutputFile ( \"gzip/${input.filename()}.gz\" ) command = \"\"\" mkdir - p / data / gzip gzip / data / $ { input . path } > / data / gzip / $ { input . filename ()}. gz \"\"\" } } Let\u2019s break this down. The main function is the application entrypoint. All it does is call the Krews run function which does all the heavy lifting, parsing command line arguments and running the application with different configurations. More on that later . Tasks \u00b6 Tasks are objects that handle processing data using docker containers. They have an input publisher and output publisher. The Input publisher may be a Mono or Flux, but the output publisher is always a Flux. If you read the Project Reactor Documentation, you may have noticed most operations described in terms of marble diagrams (See Flux docs for examples). Tasks can be conceptualized in a similar way. Tasks operate on every item from an input publisher, process them, and turn them into outputs for the output flux. Let\u2019s take another look at the tasks from the above workflow. val base64 = task < Int , File >( \"base64\" , range ) { // Everything in this scope is calculated per-input dockerImage = \"alpine:3.8\" output = OutputFile ( \"base64/$input.b64\" ) command = \"\"\" mkdir - p / data / base64 echo \"Hello world number $input!\" | base64 > / data / base64 / $ input . b64 \"\"\" } Task Inputs and Outputs \u00b6 Tasks are declared in Workflows using the task function. This function has two generic fields that we must provide with classes (The stuff in <>). The first represents the type of the Input and the second Represents the type for the output. These classes may include: Primitive types like Int, String, and Boolean Krews Files Collection classes like List, Set, and Map Data Classes Any combination of the above. For example, List of a data class objects. It\u2019s a good idea to always use Data Classes, even if you\u2019re only wrapping single Files or values. More on this below. Task Definitions \u00b6 Tasks also require 2 fields, the name and the input publisher, and a builder function (everything in {}). This function runs for every element provider by the input publisher. This input element is available as the variable \u201cinput\u201d in the function. Within the function, you may set the following fields: dockerImage (Required): The name of the docker image the task will run with. dockerDataDir: The working directory that should contain input and output files. output (Required): The output object. (More on this below) command: The command that will be executed on the given docker image. env: a map of optional environment variables to run the docker container with. cpus: number of cpus required for each task run. memory: amount of memory required for each task run. diskSize: disk space required for each task run. time: time required for each task run. Files \u00b6 Caution Krews Files in this section refer to File classes in the package krews.file NOT java.io . Make sure to import accordingly with: import krews.file.* Krews File objects are references to files that will be moved in and out of containers and File Storage. This is accomplished by using them in task Input and Output types. This can happen 3 ways: as the types directly task < File , File >( \"example\" , input ) { /* ... */ } in Collection Types task < List < File >, Map < String , File >>( \"example\" , input ) { /* ... */ } as fields in Data Classes data class MyTaskInput ( fileA : File , fileB : File ) data class MyTaskOutput ( fileX : File , fileY : File ) task < MyTaskInput , MyTaskOutput >( \"example\" , input ) { /* ... */ } Again, always using data classes for task inputs and outputs is a good idea. File Paths \u00b6 Files contain a path field. This is a partial path (ie. some-dir/file.txt ) that can be used to determine real file paths as the file gets copied to and from docker containers. When referencing a file in a task docker container, use the File.dockerPath utility to get the real path inside docker. This path will be equal to \"${task.dockerDataDir}/${file.path}\" Files also contain the following utility functions: parentDir(): gets the parent directory in the File\u2019s partial path if one exists. filename(): gets the filename without parent directories. filenameNoExt(): gets the filename without extensions Input Files \u00b6 In Krews, InputFile is a type of File . They refer to files that were not created by Krews workflows. Passing them to tasks in input element will trigger copying the file into the task\u2019s docker container. There are several implementation, and you can create your own. LocalInputFile refers to files on a locally accessible file system. For use with Local executor and files on NFS for Slurm Executor. GSInputFile refers to files in Google Cloud Storage. May be used with any executor as long as the machine running the task has permissions to access to it. InputFiles have a cache option that, when set to true, will cause Krews to copy them first to $workingDir/inputs , and each task that needs it will copy it from that directory instead of directly from the source. This can be useful if you have a type of input file that downloads remote files, from an FTP for example, slowly or with limited bandwidth. Output Files \u00b6 OutputFile is another implementation of File . These refer to files that are created by the Krews workflow. They live in /outputs in your execution environment\u2019s working directory. Using OutputFiles in task inputs will trigger the file to be copied into the docker container. Using OutputFiles in task outputs will cause the file to be copied out of the docker container into /outputs. Important Make sure to create an OutputFile for every file you want to saved. Any file that is not in the output object WILL NOT be saved to File Storage. Params \u00b6 Both workflows and tasks contain values that can be passed in from configuration files. We call these \u201cParams.\u201d Workflow params are passed into the workflow itself and task params are passed into each task function. Task functions run for each input, but the same task params are passed in each time. Both types of params have a required generic type (again, the stuff in <>\u2019s). This type just needs to be able to deserialize from our config. It\u2019s highly recommended that you use a data class for this as well. See the configurations page for more on this. Here\u2019s what this looks like on our previous example: import krews.core.* import krews.file.* import krews.run import reactor.core.publisher.* fun main ( args : Array < String >) = run ( sampleWorkflow , args ) // Now we're using our best practice of data classes for everything data class SampleParams ( val rangeMax : Int ) data class Base64Params ( val msg : String ) data class Base64Input ( val index : Int ) data class Base64Output ( val base64File : File ) data class ZipParams ( val filenamePrefix : String ) data class ZipInput ( val base64File : File ) data class ZipOutput ( val zipFile : File ) val sampleWorkflow = workflow ( \"sample-workflow\" ) { // Here's our workflow level params val params = params < SampleParams >() val base64In = ( 1. . params . rangeMax ). toFlux (). map { Base64Input ( it ) } val base64 = task < Base64Input , Base64Output >( \"base64\" , base64In ) { // and here's our workflow level params val taskParams = taskParams < Base64Params >() dockerImage = \"alpine:3.8\" output = Base64Output ( OutputFile ( \"base64/${input.index}.b64\" )) command = \"\"\" mkdir - p / data / base64 echo \"${taskParams.msg} ${input.index}!\" | base64 > / data / base64 / $ { input . index }. b64 \"\"\" } val zipIn = base64 . outputPub . map { ZipInput ( it . base64File ) } task < ZipInput , ZipOutput >( \"gzip\" , zipIn ) { val taskParams = taskParams < ZipParams >() dockerImage = \"alpine:3.8\" output = ZipOutput ( OutputFile ( \"gzip/${input.base64File.filename()}.gz\" )) command = \"\"\" mkdir - p / data / gzip gzip / data / $ { input . base64File . path } > / data / gzip / $ { taskParams . filenamePrefix }- $ { input . base64File . filename ()}. gz \"\"\" } } Code Organization \u00b6 By now you might be thinking that our workflow is starting to look pretty busy, and as our workflow grows, this problem would only get worse. Because this is an ordinary Kotlin project, we get to split our code up into multiple files. Here\u2019s how we recommend you do it. Notice we\u2019ve broken up our application into a top-level entrypoint file containing our workflow App.kt , and a file for each task in the task package. Let\u2019s take a look at one of the tasks first. task/Base64.kt package task import krews.core.* import krews.file.* import org.reactivestreams.Publisher data class Base64Params ( val msg : String ) data class Base64Input ( val index : Int ) data class Base64Output ( val base64File : File ) fun WorkflowBuilder . base64Task ( i : Publisher < Base64Input >) = this . task < Base64Input , Base64Output >( \"base64\" , i ) { val taskParams = taskParams < Base64Params >() val msg = taskParams . msg val index = input . index dockerImage = \"alpine:3.9\" output = Base64Output ( OutputFile ( \"base64/$index.b64\" )) command = \"\"\" mkdir - p / data / base64 echo \"$msg $index!\" | base64 > / data / base64 / $ index . b64 \"\"\" } Now all our Base64 task related code is in one place. We\u2019ve also used a Kotlin Extension Function on a class called WorkflowBuilder for our task creation function itself. WorkflowBuilder the class used under-the-hood when you call the \u201cworkflow\u201d function. This is just some syntax sugar that allows us to make our workflow look like this: App.kt import krews.core.* import krews.run import reactor.core.publisher.* import task.* fun main ( args : Array < String >) = run ( sampleWorkflow , args ) data class SampleParams ( val rangeMax : Int ) val sampleWorkflow = workflow ( \"sample-workflow\" ) { val params = params < SampleParams >() val base64In = ( 1. . params . rangeMax ). toFlux (). map { Base64Input ( it ) } // Here's our base64Task call referencing our extension function val base64 = base64Task ( base64In ) val zipIn = base64 . outputPub . map { ZipInput ( it . base64File ) } zipTask ( zipIn ) } Now our application file is just concerned with a higher level view of the workflow, what tasks are added and how they\u2019ve been piped together.","title":"Workflow Basics"},{"location":"workflows/#workflows","text":"A \u201cWorkflow\u201d is a directed acyclic graph consisting of \u201cTasks\u201d glued together with publishers. The following simple example contains two tasks. import krews.core.* import krews.file.* import krews.run import reactor.core.publisher.* // Bootstrap fun main ( args : Array < String >) = run ( sampleWorkflow , args ) // The application's workflow, named \"sample-workflow\" val sampleWorkflow = workflow ( \"sample-workflow\" ) { // A simple flux of the integers 1 through 10 val range = ( 1. . 10 ). toFlux () // A task that creates base64 files val base64 = task < Int , File >( \"base64\" , range ) { dockerImage = \"alpine:3.8\" output = OutputFile ( \"base64/$input.b64\" ) command = \"\"\" mkdir - p / data / base64 echo \"Hello world number $$input!\" | base64 > / data / base64 / $ input . b64 \"\"\" } // A task that zips files task < File , File >( \"gzip\" , base64 . outputPub ) { dockerImage = \"alpine:3.8\" output = OutputFile ( \"gzip/${input.filename()}.gz\" ) command = \"\"\" mkdir - p / data / gzip gzip / data / $ { input . path } > / data / gzip / $ { input . filename ()}. gz \"\"\" } } Let\u2019s break this down. The main function is the application entrypoint. All it does is call the Krews run function which does all the heavy lifting, parsing command line arguments and running the application with different configurations. More on that later .","title":"Workflows"},{"location":"workflows/#tasks","text":"Tasks are objects that handle processing data using docker containers. They have an input publisher and output publisher. The Input publisher may be a Mono or Flux, but the output publisher is always a Flux. If you read the Project Reactor Documentation, you may have noticed most operations described in terms of marble diagrams (See Flux docs for examples). Tasks can be conceptualized in a similar way. Tasks operate on every item from an input publisher, process them, and turn them into outputs for the output flux. Let\u2019s take another look at the tasks from the above workflow. val base64 = task < Int , File >( \"base64\" , range ) { // Everything in this scope is calculated per-input dockerImage = \"alpine:3.8\" output = OutputFile ( \"base64/$input.b64\" ) command = \"\"\" mkdir - p / data / base64 echo \"Hello world number $input!\" | base64 > / data / base64 / $ input . b64 \"\"\" }","title":"Tasks"},{"location":"workflows/#task-inputs-and-outputs","text":"Tasks are declared in Workflows using the task function. This function has two generic fields that we must provide with classes (The stuff in <>). The first represents the type of the Input and the second Represents the type for the output. These classes may include: Primitive types like Int, String, and Boolean Krews Files Collection classes like List, Set, and Map Data Classes Any combination of the above. For example, List of a data class objects. It\u2019s a good idea to always use Data Classes, even if you\u2019re only wrapping single Files or values. More on this below.","title":"Task Inputs and Outputs"},{"location":"workflows/#task-definitions","text":"Tasks also require 2 fields, the name and the input publisher, and a builder function (everything in {}). This function runs for every element provider by the input publisher. This input element is available as the variable \u201cinput\u201d in the function. Within the function, you may set the following fields: dockerImage (Required): The name of the docker image the task will run with. dockerDataDir: The working directory that should contain input and output files. output (Required): The output object. (More on this below) command: The command that will be executed on the given docker image. env: a map of optional environment variables to run the docker container with. cpus: number of cpus required for each task run. memory: amount of memory required for each task run. diskSize: disk space required for each task run. time: time required for each task run.","title":"Task Definitions"},{"location":"workflows/#files","text":"Caution Krews Files in this section refer to File classes in the package krews.file NOT java.io . Make sure to import accordingly with: import krews.file.* Krews File objects are references to files that will be moved in and out of containers and File Storage. This is accomplished by using them in task Input and Output types. This can happen 3 ways: as the types directly task < File , File >( \"example\" , input ) { /* ... */ } in Collection Types task < List < File >, Map < String , File >>( \"example\" , input ) { /* ... */ } as fields in Data Classes data class MyTaskInput ( fileA : File , fileB : File ) data class MyTaskOutput ( fileX : File , fileY : File ) task < MyTaskInput , MyTaskOutput >( \"example\" , input ) { /* ... */ } Again, always using data classes for task inputs and outputs is a good idea.","title":"Files"},{"location":"workflows/#file-paths","text":"Files contain a path field. This is a partial path (ie. some-dir/file.txt ) that can be used to determine real file paths as the file gets copied to and from docker containers. When referencing a file in a task docker container, use the File.dockerPath utility to get the real path inside docker. This path will be equal to \"${task.dockerDataDir}/${file.path}\" Files also contain the following utility functions: parentDir(): gets the parent directory in the File\u2019s partial path if one exists. filename(): gets the filename without parent directories. filenameNoExt(): gets the filename without extensions","title":"File Paths"},{"location":"workflows/#input-files","text":"In Krews, InputFile is a type of File . They refer to files that were not created by Krews workflows. Passing them to tasks in input element will trigger copying the file into the task\u2019s docker container. There are several implementation, and you can create your own. LocalInputFile refers to files on a locally accessible file system. For use with Local executor and files on NFS for Slurm Executor. GSInputFile refers to files in Google Cloud Storage. May be used with any executor as long as the machine running the task has permissions to access to it. InputFiles have a cache option that, when set to true, will cause Krews to copy them first to $workingDir/inputs , and each task that needs it will copy it from that directory instead of directly from the source. This can be useful if you have a type of input file that downloads remote files, from an FTP for example, slowly or with limited bandwidth.","title":"Input Files"},{"location":"workflows/#output-files","text":"OutputFile is another implementation of File . These refer to files that are created by the Krews workflow. They live in /outputs in your execution environment\u2019s working directory. Using OutputFiles in task inputs will trigger the file to be copied into the docker container. Using OutputFiles in task outputs will cause the file to be copied out of the docker container into /outputs. Important Make sure to create an OutputFile for every file you want to saved. Any file that is not in the output object WILL NOT be saved to File Storage.","title":"Output Files"},{"location":"workflows/#params","text":"Both workflows and tasks contain values that can be passed in from configuration files. We call these \u201cParams.\u201d Workflow params are passed into the workflow itself and task params are passed into each task function. Task functions run for each input, but the same task params are passed in each time. Both types of params have a required generic type (again, the stuff in <>\u2019s). This type just needs to be able to deserialize from our config. It\u2019s highly recommended that you use a data class for this as well. See the configurations page for more on this. Here\u2019s what this looks like on our previous example: import krews.core.* import krews.file.* import krews.run import reactor.core.publisher.* fun main ( args : Array < String >) = run ( sampleWorkflow , args ) // Now we're using our best practice of data classes for everything data class SampleParams ( val rangeMax : Int ) data class Base64Params ( val msg : String ) data class Base64Input ( val index : Int ) data class Base64Output ( val base64File : File ) data class ZipParams ( val filenamePrefix : String ) data class ZipInput ( val base64File : File ) data class ZipOutput ( val zipFile : File ) val sampleWorkflow = workflow ( \"sample-workflow\" ) { // Here's our workflow level params val params = params < SampleParams >() val base64In = ( 1. . params . rangeMax ). toFlux (). map { Base64Input ( it ) } val base64 = task < Base64Input , Base64Output >( \"base64\" , base64In ) { // and here's our workflow level params val taskParams = taskParams < Base64Params >() dockerImage = \"alpine:3.8\" output = Base64Output ( OutputFile ( \"base64/${input.index}.b64\" )) command = \"\"\" mkdir - p / data / base64 echo \"${taskParams.msg} ${input.index}!\" | base64 > / data / base64 / $ { input . index }. b64 \"\"\" } val zipIn = base64 . outputPub . map { ZipInput ( it . base64File ) } task < ZipInput , ZipOutput >( \"gzip\" , zipIn ) { val taskParams = taskParams < ZipParams >() dockerImage = \"alpine:3.8\" output = ZipOutput ( OutputFile ( \"gzip/${input.base64File.filename()}.gz\" )) command = \"\"\" mkdir - p / data / gzip gzip / data / $ { input . base64File . path } > / data / gzip / $ { taskParams . filenamePrefix }- $ { input . base64File . filename ()}. gz \"\"\" } }","title":"Params"},{"location":"workflows/#code-organization","text":"By now you might be thinking that our workflow is starting to look pretty busy, and as our workflow grows, this problem would only get worse. Because this is an ordinary Kotlin project, we get to split our code up into multiple files. Here\u2019s how we recommend you do it. Notice we\u2019ve broken up our application into a top-level entrypoint file containing our workflow App.kt , and a file for each task in the task package. Let\u2019s take a look at one of the tasks first. task/Base64.kt package task import krews.core.* import krews.file.* import org.reactivestreams.Publisher data class Base64Params ( val msg : String ) data class Base64Input ( val index : Int ) data class Base64Output ( val base64File : File ) fun WorkflowBuilder . base64Task ( i : Publisher < Base64Input >) = this . task < Base64Input , Base64Output >( \"base64\" , i ) { val taskParams = taskParams < Base64Params >() val msg = taskParams . msg val index = input . index dockerImage = \"alpine:3.9\" output = Base64Output ( OutputFile ( \"base64/$index.b64\" )) command = \"\"\" mkdir - p / data / base64 echo \"$msg $index!\" | base64 > / data / base64 / $ index . b64 \"\"\" } Now all our Base64 task related code is in one place. We\u2019ve also used a Kotlin Extension Function on a class called WorkflowBuilder for our task creation function itself. WorkflowBuilder the class used under-the-hood when you call the \u201cworkflow\u201d function. This is just some syntax sugar that allows us to make our workflow look like this: App.kt import krews.core.* import krews.run import reactor.core.publisher.* import task.* fun main ( args : Array < String >) = run ( sampleWorkflow , args ) data class SampleParams ( val rangeMax : Int ) val sampleWorkflow = workflow ( \"sample-workflow\" ) { val params = params < SampleParams >() val base64In = ( 1. . params . rangeMax ). toFlux (). map { Base64Input ( it ) } // Here's our base64Task call referencing our extension function val base64 = base64Task ( base64In ) val zipIn = base64 . outputPub . map { ZipInput ( it . base64File ) } zipTask ( zipIn ) } Now our application file is just concerned with a higher level view of the workflow, what tasks are added and how they\u2019ve been piped together.","title":"Code Organization"}]}